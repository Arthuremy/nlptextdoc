{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# nlptextdoc library source code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Prepare the Python environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Install Anaconda and create a virtual environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download and install Anaconda for your platform : [Anaconda - Python 3.7](https://www.anaconda.com/distribution/#download-section)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Launch Anaconda Prompt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> conda create --name nlptextenv\n",
    "\n",
    "> conda activate nlptextenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Install pandas with pyarrow.feather file format support"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> conda install pandas\n",
    "\n",
    "> conda install pyarrow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure your version of pandas is > 0.24 and pyarrow is installed :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.show_versions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Install spaCy with french language support"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> conda install -c conda-forge spacy\n",
    "\n",
    "> python -m spacy download fr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure your version of spacy is > 2.1 and fr model is installed :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================== Info about spaCy ==============================\n",
      "\n",
      "spaCy version    2.1.3                         \n",
      "Location         C:\\Users\\laure\\Anaconda3\\envs\\spacy\\lib\\site-packages\\spacy\n",
      "Platform         Windows-10-10.0.18362-SP0     \n",
      "Python version   3.7.3                         \n",
      "Models           fr                            \n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install a spaCy language detector extension :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> pip install spacy-langdetect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if the language detection works :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3.7 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'fr'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy_langdetect import LanguageDetector\n",
    "\n",
    "nlp = spacy.load(\"fr_core_news_sm\",disable=[\"tagger\",\"ner\"])\n",
    "nlp.add_pipe(LanguageDetector(), name=\"language_detector\", last=True)\n",
    "doc = nlp(\"Est-ce que le dÃ©tecteur fonctionne ?\")\n",
    "%time doc._.language[\"language\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 How to run a Jupyter notebook in the context of a conda environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> conda activate nlptextenv\n",
    "\n",
    "> conda install ipykernel\n",
    "\n",
    "> python -m ipykernel install --user --name nlptextenv --display-name \"Python (nlptextenv)\"\n",
    "\n",
    "> jupyter notebook\n",
    "\n",
    "=> menu Kernel / Change kernel / Python (nlptextenv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check : locate the Jupyter config directories, kernels are configured in the 'kernels' subdirectory, in 'kernel.json' files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\laure\\AppData\\Roaming\\jupyter\n"
     ]
    }
   ],
   "source": [
    "from jupyter_core.paths import jupyter_data_dir\n",
    "print(jupyter_data_dir())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check : locate the python environment in use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\laure\\Anaconda3\\envs\\spacy\\python.exe\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Define technical utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def _memory_size(obj, seen=None):\n",
    "    size = sys.getsizeof(obj)\n",
    "    if seen is None:\n",
    "        seen = set()\n",
    "    obj_id = id(obj)\n",
    "    if obj_id in seen:\n",
    "        return 0\n",
    "    seen.add(obj_id)\n",
    "    if isinstance(obj, dict):\n",
    "        size += sum([_memory_size(v, seen) for v in obj.values()])\n",
    "        size += sum([_memory_size(k, seen) for k in obj.keys()])\n",
    "    elif hasattr(obj, '__dict__'):\n",
    "        size += _memory_size(obj.__dict__, seen)\n",
    "    elif hasattr(obj, '__iter__') and not isinstance(obj, (str, bytes, bytearray)):\n",
    "        size += sum([_memory_size(i, seen) for i in obj])\n",
    "    return size\n",
    "\n",
    "# OTHER OPTION specific to pandas dataframes\n",
    "# https://www.dataquest.io/blog/pandas-big-data/\n",
    "# df.info(memory_usage=\"deep\")\n",
    "\n",
    "def _file_size(filepath):\n",
    "    statinfo = os.stat(filepath)\n",
    "    return statinfo.st_size\n",
    "\n",
    "def _format_size_mb(size):\n",
    "    return int(size / 1024.0 / 102.4) / 10.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prepare the .NET environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Install Visual Studio 2019 community"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download and install [Microsoft Visual Studio 2019](https://visualstudio.microsoft.com/fr/downloads/) community edition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install only the following workload : .NET Core multiplatform development."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Clone and compile nlptextdoc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Launch Visual Studio 2019.\n",
    "\n",
    "Clone code\n",
    "- Repository URL : https://github.com/laurentprudhon/nlptextdoc.git\n",
    "- Choose a local directory for the solution\n",
    "\n",
    "Double click on the solution file : nlptextdoc.sln\n",
    "\n",
    "Select the \"Release\" configuration in the top toolbar.\n",
    "\n",
    "In the Solution Explorer :\n",
    "- right-click on the solution root => Generate solution\n",
    "- right-click on the projet \"nlptextdoc.cli\" => Open directory in File Explorer\n",
    "\n",
    "Navigate to the bin\\Release\\netcoreapp2.1 subdirectory :\n",
    "- this directory should contain 7 .dll files, including : nlptextdoc.cli.dll\n",
    "- copy the full path of this directory in the variable below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlptextdocExecPath = r\"C:\\Users\\laure\\source\\repos\\nlptextdoctemp\\nlptextdoc.cli\\bin\\Release\\netcoreapp2.1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the command line client and learn its syntax :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nlptexdoc extractor v1.0\n",
      "\n",
      "Crawls all the Html pages of a website and converts them to .nlp.txt structured text documents.\n",
      "All the extracted text documents are stored under a single directory named like the website.\n",
      "The .nlp.txt file format is described here : https://www.cognitivefactory.org/nlptextdocument/\n",
      "\n",
      "Features an advanced Html to text conversion algorithm :\n",
      "- tries to recover the logical structure of the document from the Html layout\n",
      "- interprets Css properties of the Html nodes to make this operation more reliable\n",
      "- preserves document / section / list / table grouping and nesting information\n",
      "\n",
      "Usage : nlptextdoc [rootUrl] [storageDirectory] [maxPagesCount=0] [minCrawlDelay=0]\n",
      " - rootUrl          : root Url of the website (or subfolder of a website) you want to crawl\n",
      " - storageDirectory : path to the disk directory where the website folder\n",
      " - maxPagesCount    : maximum number of pages extracted from the website (optional, default:100 000)\n",
      " - minCrawlDelay    : delay in milliseconds between two requests sent to the website (optional, default:100ms)\n",
      "\n",
      "Recommended process :\n",
      "1. Run the the tool for the first time with a low maxPagesCount (for example 500) and no crawl delay\n",
      "2. Open the log file \"httprequests.log.csv\" (created at the root of the website directory) in a spreadsheet\n",
      "3. Check for Http \"Forbidden\" answers, and test if the url is accessible when tested from a browser\n",
      "4. Try again with a non null minCrawlDelay, and continue to inscrease it until \"Forbidden\" errors disappear\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!dotnet \"{nlptextdocExecPath}/nlptextdoc.cli.dll\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Extract nlp text documents from websites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Identify popular websites to build your specific language model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List the public and open websites you would like to read to build a language model.\n",
    "\n",
    "PLEASE MAKE SURE THIS IS LEGAL in your country.\n",
    "\n",
    "For example in Europe : https://ec.europa.eu/digital-single-market/en/modernisation-eu-copyright-rules. \n",
    "\n",
    "> \"The mandatory exceptions that the proposed directive announced are related to: ... Text and data mining ...\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "websites = [\"http://bourse.latribune.fr/\",\n",
    "            \"http://cercledelepargne.com/\",\n",
    "            \"http://finance.lelynx.fr/banques/\",\n",
    "            \"http://labourseauquotidien.fr/\",\n",
    "            \"http://lafourmiz.fr/\",\n",
    "            \"http://www.assurances.com/\",\n",
    "            \"http://www.banque.org/\",\n",
    "            \"http://www.banque-info.com/\",\n",
    "            \"http://www.bourse.fr/\",\n",
    "            \"http://www.boursedirect.fr/\",\n",
    "            \"http://www.capitaine-epargne.com/\",\n",
    "            \"http://www.cnp.fr/\",\n",
    "            \"http://www.cofinoga.fr/\",\n",
    "            \"http://www.comparabanques.fr/\",\n",
    "            \"http://www.comparalivrets.fr/\",\n",
    "            \"http://www.fbf.fr/\",\n",
    "            \"http://www.financo.fr/\",\n",
    "            \"http://www.generali.fr/\",\n",
    "            \"http://www.guide-epargne.com/\",\n",
    "            \"http://www.lemonde.fr/epargne/\",\n",
    "            \"http://www.leparisien.fr/actus/banque\",\n",
    "            \"http://www.lesaffaires.com/bourse\",\n",
    "            \"http://www.lesclesdelabanque.com\",\n",
    "            \"http://www.msn.com/fr-fr/finance\",\n",
    "            \"http://www.retraiteepargne.fr/\",\n",
    "            \"http://www.revue-banque.fr/\",\n",
    "            \"http://www.strategie-bourse.com/\",\n",
    "            \"http://www.zonebourse.com/\",\n",
    "            \"https://acpr.banque-france.fr/\",\n",
    "            \"https://banque.meilleurtaux.com/\",\n",
    "            \"https://bourse.lefigaro.fr/\",\n",
    "            \"https://compte-nickel.fr/\",\n",
    "            \"https://eko-by-ca.fr/\",\n",
    "            \"https://epargne.ooreka.fr/\",\n",
    "            \"https://ffa-assurance.fr/\",\n",
    "            \"https://fr.finance.yahoo.com/\",\n",
    "            \"https://humanis.com/\",\n",
    "            \"https://mabanque.bnpparibas/\",\n",
    "            \"https://mes-placements.fr/\",\n",
    "            \"https://n26.com/fr-fr/\",\n",
    "            \"https://particulier.apicil.com/\",\n",
    "            \"https://www.10meilleuresbanques.fr/\",\n",
    "            \"https://www.abcbourse.com/\",\n",
    "            \"https://www.afer.fr/\",\n",
    "            \"https://www.ag2rlamondiale.fr/\",\n",
    "            \"https://www.agpm.fr/\",\n",
    "            \"https://www.allianz.fr/\",\n",
    "            \"https://www.allianzbanque.fr/\",\n",
    "            \"https://www.amaguiz.com/\",\n",
    "            \"https://www.ameli.fr/\",\n",
    "            \"https://www.amundi.fr/fr_part\",\n",
    "            \"https://www.arkea.com/\",\n",
    "            \"https://www.assurland.com/\",\n",
    "            \"https://www.aviva.fr/\",\n",
    "            \"https://www.axa.fr/\",\n",
    "            \"https://www.banque.fr/\",\n",
    "            \"https://www.banque-casino.fr/\",\n",
    "            \"https://www.banque-edel.fr/\",\n",
    "            \"https://www.banque-france.fr/\",\n",
    "            \"https://www.banquepopulaire.fr/\",\n",
    "            \"https://www.banquesenligne.org/\",\n",
    "            \"https://www.bforbank.com/\",\n",
    "            \"https://www.boursedeparis.fr/\",\n",
    "            \"https://www.boursier.com/\",\n",
    "            \"https://www.boursorama.com/\",\n",
    "            \"https://www.boursorama-banque.com/\",\n",
    "            \"https://www.bred.fr/\",\n",
    "            \"https://www.ca-alsace-vosges.fr/\",\n",
    "            \"https://www.caisse-epargne.fr/\",\n",
    "            \"https://www.carrefour-banque.fr/\",\n",
    "            \"https://www.cbanque.com/\",\n",
    "            \"https://www.cetelem.fr/\",\n",
    "            \"https://www.challenges.fr/tag_theme/banque_876/\",\n",
    "            \"https://www.cic.fr/\",\n",
    "            \"https://www.cofidis.fr/\",\n",
    "            \"https://www.credit-cooperatif.coop/\",\n",
    "            \"https://www.credit-du-nord.fr/\",\n",
    "            \"https://www.credit-et-banque.com/\",\n",
    "            \"https://www.creditfoncier.fr/\",\n",
    "            \"https://www.creditmutuel.fr/\",\n",
    "            \"https://www.culturebanque.com/\",\n",
    "            \"https://www.diac.fr/\",\n",
    "            \"https://www.direct-assurance.fr/\",\n",
    "            \"https://www.economie.gouv.fr/\",\n",
    "            \"https://www.empruntis.com/epargne/\",\n",
    "            \"https://www.en-bourse.fr/\",\n",
    "            \"https://www.eurofil.com/\",\n",
    "            \"https://www.fortuneo.fr/\",\n",
    "            \"https://www.francetransactions.com/\",\n",
    "            \"https://www.gan.fr/\",\n",
    "            \"https://www.groupama.fr/\",\n",
    "            \"https://www.hellobank.fr/\",\n",
    "            \"https://www.home.saxo/fr-fr/\",\n",
    "            \"https://www.hsbc.fr/\",\n",
    "            \"https://www.impots.gouv.fr/portail/\",\n",
    "            \"https://www.ing.fr/banque-en-ligne/\",\n",
    "            \"https://www.labanquepostale.fr/\",\n",
    "            \"https://www.lcl.fr/\",\n",
    "            \"https://www.lerevenu.com/\",\n",
    "            \"https://www.lesechos.fr/finance-marches/\",\n",
    "            \"https://www.lesfurets.com/\",\n",
    "            \"https://www.lolivier.fr/\",\n",
    "            \"https://www.macif.fr/assurance/particuliers\",\n",
    "            \"https://www.mae.fr/\",\n",
    "            \"https://www.maif.fr/\",\n",
    "            \"https://www.matmut.fr/\",\n",
    "            \"https://www.mma.fr/\",\n",
    "            \"https://www.monabanq.com/fr/index.html\",\n",
    "            \"https://www.mon-epargne.com/\",\n",
    "            \"https://www.montepaschi-banque.fr/fr/\",\n",
    "            \"https://www.natixis.com/\",\n",
    "            \"https://www.oney.fr/\",\n",
    "            \"https://www.orangebank.fr/\",\n",
    "            \"https://www.ouest-france.fr/economie/banques-finance/\",\n",
    "            \"https://www.palatine.fr/\",\n",
    "            \"https://www.panorabanques.com/\",\n",
    "            \"https://www.probtp.com/\",\n",
    "            \"https://www.psabanque.fr/\",\n",
    "            \"https://www.quechoisir.org/thematique-banque-credit-t111/\",\n",
    "            \"https://www.revolut.com/fr-FR/\",\n",
    "            \"https://www.service-public.fr/particuliers/vosdroits/N19803\",\n",
    "            \"https://www.smc.fr/\",\n",
    "            \"https://www.societegenerale.fr/\",\n",
    "            \"https://www.sofinco.fr/\",\n",
    "            \"https://www.toutsurmesfinances.com/\",\n",
    "            \"https://www.tradingsat.com/\",\n",
    "            \"https://www.usine-digitale.fr/banque/\",\n",
    "            \"https://www.younited-credit.com/\"]\n",
    "\n",
    "len(websites)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Extract raw text from these websites in a local directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a local directory to store the extracted nlp text documents : be careful, this directory may contain several gigabytes of data at the end of the process.\n",
    "\n",
    "IMPORTANT : **the \"magic\" \\\\\\\\?\\ prefix in the root path is mandatory on Windows** to enable long file names support in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "rootdir = Path(r\"\\\\?\\C:\\Users\\laure\\Desktop\\nlptextdoc-data-201907\")\n",
    "rootdir.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start by extracting only a few documents (for example 100) from each webiste, to test if they are accessible and if everything works as expected :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxPagesCount = 100\n",
    "\n",
    "for websiteUrl in websites:\n",
    "    !dotnet \"{nlptextdocExecPath}/nlptextdoc.cli.dll\" {websiteUrl} {str(rootdir)} {maxPagesCount}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the local root directory, the extraction program creates one subdirectory per website.\n",
    "\n",
    "Each website subdirectory contains :\n",
    "- one log file called **httprequests.log.csv**\n",
    "- subdirectories reproducing the website tree structure\n",
    "- one **nlp.txt text document** for each extracted html page in this tree structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the following page for a **description of the nlptextdoc format** : https://github.com/laurentprudhon/nlptextdoc/blob/master/README.md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if all the websites were correctly extracted :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "def getWebsiteName(websiteurl):\n",
    "    url = urlparse(websiteurl)\n",
    "    websitename = url.netloc\n",
    "    return websitename\n",
    "\n",
    "def getWebsiteDir(rootdir, websitename):\n",
    "    websitedir = rootdir / websitename\n",
    "    return websitedir\n",
    "\n",
    "def loadExtractionLogs(websitedir):\n",
    "    return pd.read_csv(websitedir / \"httprequests.log.csv\",delimiter=\";\")\n",
    "\n",
    "def getExtractionStats(websites):\n",
    "    websiteNames = []\n",
    "    requestsCount = []\n",
    "    statusCounts = []    \n",
    "    errorTypes = [\"OK\",\"NotFound\",\"Redirect\",\"NoContent\",\"Forbidden\",\"BadRequest\",\"Moved\"]\n",
    "    for errorType in errorTypes:\n",
    "        statusCounts.append([])\n",
    "    for websiteurl in websites:\n",
    "        website = getWebsiteName(websiteurl)\n",
    "        print(f\"Checking extraction logs for website {website} ...\")\n",
    "        websitedir = getWebsiteDir(rootdir, website)\n",
    "        logsdf = loadExtractionLogs(websitedir)\n",
    "        logsstatus = logsdf[\"Status code\"].value_counts()\n",
    "        websiteNames.append(website)\n",
    "        requestsCount.append(len(logsdf))\n",
    "        for idx,errorType in enumerate(errorTypes):\n",
    "            statusCounts[idx].append(logsstatus[errorType] if errorType in logsstatus else 0)\n",
    "    dictResult = {}\n",
    "    dictResult[\"Website\"] = websiteNames\n",
    "    dictResult[\"Requests\"] = requestsCount\n",
    "    for idx,errorType in enumerate(errorTypes):\n",
    "        dictResult[errorType] = statusCounts[idx]\n",
    "    return pd.DataFrame(dictResult)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extractionStats = getExtractionStats(websites)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extractionStats[extractionStats[\"Requests\"] != extractionStats[\"OK\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each website with http error codes, open the log file **httprequests.log.csv** and see if something needs to be fixed.\n",
    "\n",
    "Use the code below to test if the errors :\n",
    "- were temporary, a consequence of a the high request rate the extraction program : then relaunch the extraction of the website with a bigger minCrawlDelay\n",
    "- are a real problem in the source website : just ignore them and continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from urllib.error import HTTPError\n",
    "\n",
    "def checkExtractionLogsByErrorType(logsdf):\n",
    "    errorTypes = [\"NotFound\",\"Redirect\",\"NoContent\",\"Forbidden\",\"BadRequest\",\"Moved\"]\n",
    "    urls = []\n",
    "    extractionStatus = []\n",
    "    checkedStatus = []\n",
    "    for errorType in errorTypes:\n",
    "        urlsWithError = logsdf[logsdf[\"Status code\"] == errorType][\"Url\"]\n",
    "        print(f\"Testing {len(urlsWithError)} URLs with error type {errorType} ...\")\n",
    "        for url in urlsWithError:\n",
    "            urls.append(url)\n",
    "            extractionStatus.append(errorType)\n",
    "            try:\n",
    "                resp = urlopen(url)\n",
    "                checkedStatus.append(resp.getcode())\n",
    "            except HTTPError as he:\n",
    "                checkedStatus.append(he.code)\n",
    "    checksdf = pd.DataFrame({\"Urls\" : urls, \"ExtractionStatus\" : extractionStatus, \"CheckedStatus\" : checkedStatus})\n",
    "    return checksdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "www.boursedirect.fr\n",
      "Testing 0 URLs with error type NotFound ...\n",
      "Testing 1 URLs with error type Redirect ...\n",
      "Testing 0 URLs with error type NoContent ...\n",
      "Testing 2 URLs with error type Forbidden ...\n",
      "Testing 0 URLs with error type BadRequest ...\n",
      "Testing 0 URLs with error type Moved ...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Urls</th>\n",
       "      <th>ExtractionStatus</th>\n",
       "      <th>CheckedStatus</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://www.boursedirect.fr/priv/logoutPriv.php</td>\n",
       "      <td>Redirect</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://www.boursedirect.fr/fr/profil</td>\n",
       "      <td>Forbidden</td>\n",
       "      <td>403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://www.boursedirect.fr/fr/messagerie</td>\n",
       "      <td>Forbidden</td>\n",
       "      <td>403</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Urls ExtractionStatus  \\\n",
       "0  http://www.boursedirect.fr/priv/logoutPriv.php         Redirect   \n",
       "1            http://www.boursedirect.fr/fr/profil        Forbidden   \n",
       "2        http://www.boursedirect.fr/fr/messagerie        Forbidden   \n",
       "\n",
       "   CheckedStatus  \n",
       "0            200  \n",
       "1            403  \n",
       "2            403  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "websiteIndex = 9\n",
    "websitename = getWebsiteName(websites[websiteIndex])\n",
    "print(websitename)\n",
    "\n",
    "websitedir = getWebsiteDir(rootdir, websitename)\n",
    "logsdf = loadExtractionLogs(websitedir)\n",
    "checkExtractionLogsByErrorType(logsdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When everything seems OK, relaunch the extraction code above with a much bigger maxPagesCount (for example 100 000)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Download publicly available french dictionaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a local subdirectory to store the french dictionaries :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictdir = rootdir / \"_dictionaries\"\n",
    "dictdir.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dictionary 1 : Dicollecte** - Open Source french dictionary for LibreOffice/OpenOffice\n",
    "\n",
    "Website : https://grammalecte.net/home.php?prj=fr.\n",
    "\n",
    "Licence : MPL : Mozilla Public License version 2.0  -  http://www.mozilla.org/MPL/2.0/.\n",
    "\n",
    "Download the latest \"Lexique\" on the [Grammalecte downloads page](https://grammalecte.net/download.php?prj=fr) :\n",
    "- open the zip file\n",
    "- copy only the \"lexique-dicollecte-fr-v*.txt file (for example : lexique-dicollecte-fr-v6.4.1.txt) in the local subdirectory\n",
    "\n",
    "Open the file in a text editor to see its self-descriptive format and contents.\n",
    "\n",
    "Store the file name in the variable below :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dicollectefile = dictdir / \"lexique-dicollecte-fr-v6.4.1.txt\"\n",
    "dicollectefile.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildDicollecteTags(dicollectefile):\n",
    "    dictionarydf = pd.read_csv(dicollectefile, sep=\"\\t\", skiprows=15)\n",
    "    dictionarydf.head()\n",
    "    dictionarytags = {}\n",
    "    for index, row in dictionarydf.iterrows():\n",
    "        token = row[\"Flexion\"]\n",
    "        tag = _convertDicollecteTagsToUnivDepTags(row[\"Ãtiquettes\"])\n",
    "        if(not (token in dictionarytags)):\n",
    "            dictionarytags[token] = tag\n",
    "        elif(not (tag in dictionarytags[token])):\n",
    "            dictionarytags[token] = dictionarytags[token] + \"|\" + tag\n",
    "    return dictionarytags\n",
    "\n",
    "def _convertDicollecteTagsToUnivDepTags(text):\n",
    "    if((\"adj\" in text) or (\"loc.adj\" in text)):\n",
    "        return \"ADJ\"\n",
    "    elif(\"prep\" in text):\n",
    "        return \"ADP\"\n",
    "    elif((\"adv\" in text) or (\"loc.adv\" in text)):\n",
    "        return \"ADV\"\n",
    "    elif((\"v0a\" in text) or (\"v0e\" in text) or (\"ppas\" in text)):\n",
    "        return \"AUX\"\n",
    "    elif(\"cjco\" in text):\n",
    "        return \"CCONJ\"\n",
    "    elif(\"det\" in text):\n",
    "        return \"DET\"\n",
    "    elif(\"interj\" in text):\n",
    "        return \"INTJ\"\n",
    "    elif(\"nom\" in text):\n",
    "        return \"NOUN\"\n",
    "    elif((\"nb\" in text) or (\"ord\" in text)):\n",
    "        return \"NUM\"\n",
    "    elif(\"pro\" in text):\n",
    "        return \"PRON\"\n",
    "    elif((\"prn\" in text) or (\"patr\" in text) or (\"npr\" in text)):\n",
    "        return \"PROPN\"\n",
    "    elif(\"cjsub\" in text):\n",
    "        return \"SCONJ\"\n",
    "    elif(\"symb\" in text):\n",
    "        return \"SYM\"\n",
    "    elif((\"v1\" in text) or (\"v2\" in text) or (\"v3\" in text) or (\"loc.verb\" in text)):\n",
    "        return \"VERB\"\n",
    "    else:\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dicollecteTags = buildDicollecteTags(dicollectefile)\n",
    "dicollecteTags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dictionary 2 : UDLexicons Lefff** - Research resource from INRIA for the [Universal Dependencies](https://universaldependencies.org/) project\n",
    "\n",
    "Citation : BenoÃ®t Sagot. A multilingual collection of CoNLL-U-compatible morphological lexicons. Eleventh International Conference on Language Resources and Evaluation (LREC 2018), May 2018, Miyazaki, Japan. hal-01798798v2\n",
    "\n",
    "Paper : https://hal.inria.fr/hal-01798798v2/document\n",
    "\n",
    "Download the latest \"UDLexicons\" on [BenoÃ®t Sagot's resources page](http://alpage.inria.fr/~sagot/) :\n",
    "- open the zip file\n",
    "- copy only the \"UDLex_French-Lefff.conllul\" in the local directory\n",
    "- add a .txt extension to the file name\n",
    "\n",
    "Open the file in a text editor to see its self-descriptive format and contents.\n",
    "\n",
    "Store the file name in the variable below :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "leffffile = dictdir / \"UDLex_French-Lefff.conllul.txt\"\n",
    "leffffile.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildLefffTags(leffffile):\n",
    "    lexicondf = pd.read_csv(leffffile, sep=\"\\t\")\n",
    "    lexicontags = {}\n",
    "    for index, row in lexicondf.iterrows():\n",
    "        token = row[\"!\"]\n",
    "        tag = row[\"PUNCT\"]\n",
    "        if(not (token in lexicontags)):\n",
    "            lexicontags[token] = tag\n",
    "        elif(not (tag in lexicontags[token])):\n",
    "            lexicontags[token] = lexicontags[token] + \"|\" + tag\n",
    "    return lexicontags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lefffTags = buildLefffTags(leffffile)\n",
    "leffftags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Generate text dataset, statistics, dictionaries from websites extraction directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Load the extracted text files in an efficient DataFrame for each website"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following class can be used to **parse and load the .nlp.txt text files extracted from a website in a DataFrame**.\n",
    "\n",
    "See the following page for a **description of the nlptextdoc format** : https://github.com/laurentprudhon/nlptextdoc/blob/master/README.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "\n",
    "class NLPTextDocumentReader:\n",
    "    \"\"\"Read output files of a website extraction in pandas DataFrames.\n",
    "    \n",
    "    Sample usage :\n",
    "    \n",
    "    textreader = NLPTextDocumentReader(websitedir)\n",
    "    textdf = textreader.load_dataframe()\n",
    "    \"\"\"    \n",
    "    def __init__(self, websitedir):\n",
    "        self.websitedir = websitedir\n",
    "        \n",
    "        self.documentCount = 0 \n",
    "        self.nestingLevel = 1\n",
    "        self.listType = []\n",
    "        self.listCmd = []\n",
    "        self.listLevel = []\n",
    "        self.listText = []\n",
    "                \n",
    "        self.DOCUMENT_ELEMENT_LINE_MARKER = \"##\"\n",
    "        self.DOCUMENT_ELEMENT_START = \"Start\"\n",
    "        self.DOCUMENT_ELEMENT_END = \"End\"\n",
    "        self.DOCUMENT_ELEMENT_ITEMS = \"Items\"\n",
    "        self.DOCUMENT_ELEMENT_ITEMS_START = \">>\"\n",
    "        self.DOCUMENT_ELEMENT_ITEMS_SEPARATOR = \"||\"\n",
    "        \n",
    "        self.TEXT_DOCUMENT_PROPERTY_PREFIX = self.DOCUMENT_ELEMENT_LINE_MARKER + \" NLPTextDocument \"\n",
    "        self.TEXT_DOCUMENT_TITLE = \"Title\"\n",
    "        self.TEXT_DOCUMENT_URI = \"Uri\"\n",
    "        \n",
    "        self.DOCUMENT_ELEMENT_LINE_REGEX = re.compile(\n",
    "            self.DOCUMENT_ELEMENT_LINE_MARKER + \" \"\n",
    "            + \"(?P<NestingLevel>[0-9]+)\" + \" \"\n",
    "            + \"(?P<ElementName>[A-Za-z]+)\" + \" \"\n",
    "            + \"(?P<Command>\" + self.DOCUMENT_ELEMENT_START + \"|\" + self.DOCUMENT_ELEMENT_END + \"|\" + self.DOCUMENT_ELEMENT_ITEMS + \")\" + \" ?\")\n",
    "        \n",
    "    def load_dataframe(self):\n",
    "        textdffile = self.websitedir / \"nlptextdocs.dataframe.feather\"\n",
    "        if(textdffile.exists()):\n",
    "            return pd.read_feather(textdffile)\n",
    "        else:\n",
    "            for textfile in self.websitedir.glob(\"**/*.nlp.txt\"):\n",
    "                with textfile.open(mode=\"r\", encoding=\"utf-8-sig\") as f:   \n",
    "                    self.textfile = textfile\n",
    "                    self.documentCount = self.documentCount+1\n",
    "                    self.onDocumentStart(str(self.documentCount))\n",
    "                    self.isreadingproperties = True\n",
    "                    for lineidx,line in enumerate(f):\n",
    "                        line = line.strip()\n",
    "                        if(not line): continue\n",
    "                        self.lineidx = lineidx\n",
    "                        self.readline(line)\n",
    "                    self.onDocumentEnd(str(self.documentCount))\n",
    "            textdf = pd.DataFrame({\"DocEltType\": self.listType, \"DocEltCmd\" : self.listCmd, \"NestingLevel\": self.listLevel, \"Text\":self.listText})\n",
    "            textdf = textdf.astype({\"DocEltType\": \"category\", \"DocEltCmd\": \"category\", \"NestingLevel\": np.uint8},copy=False)\n",
    "            self.__init__(self.websitedir)\n",
    "            textdf.to_feather(textdffile)\n",
    "            return textdf\n",
    "\n",
    "    def readline(self,line):\n",
    "        if (self.isreadingproperties):\n",
    "            if (line.startswith(self.TEXT_DOCUMENT_PROPERTY_PREFIX)):\n",
    "                self.readproperty(line[len(self.TEXT_DOCUMENT_PROPERTY_PREFIX):])\n",
    "            else:\n",
    "                self.isreadingproperties = False\n",
    "        if (not self.isreadingproperties):\n",
    "            self.readelement(line)\n",
    "                \n",
    "    def readproperty(self,propstr):\n",
    "        firstspaceindex = propstr.find(\" \");\n",
    "        if (firstspaceindex > 0):\n",
    "            propertyname = propstr[:firstspaceindex]            \n",
    "            propertyvalue = propstr[firstspaceindex + 1:].strip()\n",
    "            if(propertyname == self.TEXT_DOCUMENT_TITLE):\n",
    "                self.onDocumentTitle(propertyvalue)\n",
    "            elif(propertyname == self.TEXT_DOCUMENT_URI):\n",
    "                self.onDocumentUri(propertyvalue)       \n",
    "    \n",
    "    def readelement(self,line):\n",
    "        if (line.startswith(self.DOCUMENT_ELEMENT_LINE_MARKER)):\n",
    "            self.readcommand(line)\n",
    "        else:\n",
    "            self.onTextBlock(line)\n",
    "    \n",
    "    def readcommand(self,line):\n",
    "        match = self.DOCUMENT_ELEMENT_LINE_REGEX.match(line)\n",
    "        if(match): \n",
    "            self.nestingLevel = int(match.group(\"NestingLevel\"))\n",
    "            elementName = match.group(\"ElementName\")\n",
    "            command = match.group(\"Command\")\n",
    "            if (command == self.DOCUMENT_ELEMENT_START):\n",
    "                title = line[match.end():].strip()\n",
    "                if (len(title) == 0): title = None\n",
    "                if(elementName == \"Section\"):\n",
    "                    self.onSectionStart(title)\n",
    "                elif(elementName == \"NavigationList\"):\n",
    "                    self.onNavigationListStart(title)\n",
    "                elif(elementName == \"List\"):\n",
    "                    self.onListStart(title)\n",
    "                elif(elementName == \"ListItem\"):\n",
    "                    self.onListItemStart()\n",
    "                elif(elementName == \"Table\"):\n",
    "                    self.onTableStart(title)\n",
    "                elif(elementName == \"TableHeader\"):\n",
    "                    self.onTableHeaderStart()           \n",
    "                elif(elementName == \"TableCell\"):\n",
    "                    self.onTableCellStart()\n",
    "            elif (command == self.DOCUMENT_ELEMENT_END):\n",
    "                if(elementName == \"Section\"):\n",
    "                    self.onSectionEnd()\n",
    "                elif(elementName == \"NavigationList\"):\n",
    "                    self.onNavigationListEnd()\n",
    "                elif(elementName == \"List\"):\n",
    "                    self.onListEnd()\n",
    "                elif(elementName == \"ListItem\"):\n",
    "                    self.onListItemEnd()\n",
    "                elif(elementName == \"Table\"):\n",
    "                    self.onTableEnd()\n",
    "                elif(elementName == \"TableHeader\"):\n",
    "                    self.onTableHeaderEnd()                 \n",
    "                elif(elementName == \"TableCell\"):\n",
    "                    self.onTableCellEnd()\n",
    "            elif (command == self.DOCUMENT_ELEMENT_ITEMS):\n",
    "                startOfItems = line.find(self.DOCUMENT_ELEMENT_ITEMS_START)\n",
    "                title = line[match.end():startOfItems].strip()\n",
    "                if (len(title) == 0): title = None\n",
    "                if (elementName == \"NavigationList\"):\n",
    "                    self.onNavigationListStart(title)\n",
    "                elif (elementName == \"List\"):\n",
    "                    self.onListStart(title)             \n",
    "                self.nestingLevel = self.nestingLevel+1\n",
    "                items = line[startOfItems+len(self.DOCUMENT_ELEMENT_ITEMS_START):].split(self.DOCUMENT_ELEMENT_ITEMS_SEPARATOR)\n",
    "                for item in items:\n",
    "                    item = item.strip()\n",
    "                    if (len(item) > 0):\n",
    "                        self.onInlineListItem(item)\n",
    "                self.nestingLevel = self.nestingLevel-1\n",
    "                if (elementName == \"NavigationList\"):\n",
    "                    self.onNavigationListEnd()\n",
    "                elif (elementName == \"List\"):\n",
    "                    self.onListEnd()\n",
    "            else:\n",
    "                raise Exception(f\"File format error in file {self.textfile} on line {self.lineidx} : {line[:min(len(line), 50)]}\");                     \n",
    "        else:\n",
    "            raise Exception(f\"File format error in file {self.textfile} on line {self.lineidx} : {line[:min(len(line), 50)]}\");\n",
    "    \n",
    "    def onDocumentStart(self,docId):\n",
    "        self.appendrow(\"Document\",\"Start\",docId)\n",
    "    \n",
    "    def onDocumentTitle(self,title):\n",
    "        self.appendrow(\"Document\",\"Title\",title)\n",
    "            \n",
    "    def onDocumentUri(self,uri):\n",
    "        self.appendrow(\"Document\",\"Uri\",uri)\n",
    "    \n",
    "    def onDocumentEnd(self,docId):\n",
    "        self.appendrow(\"Document\",\"End\",docId)\n",
    "    \n",
    "    def onTextBlock(self,text):\n",
    "        self.appendrow(\"TextBlock\",\"Text\",text)\n",
    "            \n",
    "    def onSectionStart(self,title):\n",
    "        self.appendrow(\"Section\",\"Start\",title)\n",
    "        \n",
    "    def onSectionEnd(self): \n",
    "        self.appendrow(\"Section\",\"End\")\n",
    "        \n",
    "    def onNavigationListStart(self,title):\n",
    "        self.appendrow(\"NavigationList\",\"Start\",title)\n",
    "        \n",
    "    def onNavigationListEnd(self):\n",
    "        self.appendrow(\"NavigationList\",\"End\")\n",
    "        \n",
    "    def onListStart(self,title):\n",
    "        self.appendrow(\"List\",\"Start\",title)\n",
    "        \n",
    "    def onListEnd(self):\n",
    "        self.appendrow(\"List\",\"End\")\n",
    "        \n",
    "    def onInlineListItem(self,item):\n",
    "        self.appendrow(\"ListItem\",\"Text\",item)\n",
    "            \n",
    "    def onListItemStart(self):\n",
    "        self.appendrow(\"ListItem\",\"Start\")\n",
    "        \n",
    "    def onListItemEnd(self):\n",
    "        self.appendrow(\"ListItem\",\"End\")\n",
    "        \n",
    "    def onTableStart(self,title):\n",
    "        self.appendrow(\"Table\",\"Start\",title)\n",
    "    \n",
    "    def onTableEnd(self):\n",
    "        self.appendrow(\"Table\",\"End\")\n",
    "        \n",
    "    def onTableHeaderStart(self):\n",
    "        self.appendrow(\"TableHeader\",\"Start\")\n",
    "        \n",
    "    def onTableHeaderEnd(self): \n",
    "        self.appendrow(\"TableHeader\",\"End\")\n",
    "        \n",
    "    def onTableCellStart(self):\n",
    "        self.appendrow(\"TableCell\",\"Start\")\n",
    "        \n",
    "    def onTableCellEnd(self): \n",
    "        self.appendrow(\"TableCell\",\"End\")\n",
    "            \n",
    "    def appendrow(self,docEltType,docEltCmd,text=None):\n",
    "        self.listType.append(docEltType)\n",
    "        self.listCmd.append(docEltCmd)\n",
    "        self.listLevel.append(self.nestingLevel)\n",
    "        if(text != None):\n",
    "            text = text.replace(\"\\\\n\",\"\\n\")\n",
    "        self.listText.append(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the function below to prepare DataFrames for all the extracted websites at once :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepareDataFramesForWebsites(rootdir, websites):\n",
    "    \"\"\"Loads all individual text blocks extracted from the pages of each website in a dataframe, and save them efficiently on disk.\n",
    "\n",
    "    Parameters:\n",
    "    rootdir - Path to the directory where the websites were extracted\n",
    "    websites - List of strings with the websites root URLs\n",
    "    \"\"\"\n",
    "    for websiteurl in websites:\n",
    "        website = getWebsiteName(websiteurl)\n",
    "        print(f\"Preparing dataframe for website {website} ...\")        \n",
    "        websitedir = getWebsiteDir(rootdir,website)\n",
    "        reader = NLPTextDocumentReader(websitedir)\n",
    "        textdf = reader.load_dataframe()\n",
    "        docsCount = len(textdf[(textdf[\"DocEltType\"]==\"Document\") & (textdf[\"DocEltCmd\"]==\"Start\")])\n",
    "        logsdf = loadExtractionLogs(websitedir)\n",
    "        print(f\"- {len(logsdf)} website extraction logs\")\n",
    "        print(f\"- {docsCount} documents\")\n",
    "        print(f\"- {len(textdf)} document elements\")\n",
    "        print(f\"- dataframe size in memory : {_format_size_mb(_memory_size(textdf))} MB\")\n",
    "        websitefile = websitedir / \"nlptextdocs.dataframe.feather\"\n",
    "        print(f\"- dataframe size on disk : {_format_size_mb(_file_size(websitefile))} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepareDataFramesForWebsites(rootdir, websites)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you encounter a parsing error in any of the text files : just delete the corrupted file and relaunch the function above.\n",
    "\n",
    "It will run very efficiently for all the websites already processed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Filter and aggregate all interesting text blocks in a single DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While we filter and aggregate all the interesting text blocks in a single DataFrame, we also generate the following summaries of the text data for later use :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Information about the character set used in the extracted dataset :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveCharset(rootdir, vocabdf):\n",
    "    print(\"Saving the character set ...\")\n",
    "    charset = defaultdict(lambda:0)\n",
    "    for idx,row in vocabdf.iterrows():\n",
    "        token = row[\"Words\"]\n",
    "        count = row[\"Counts\"]\n",
    "        for char in token:\n",
    "            charcode = ord(char)\n",
    "            charcounts[charcode] = charcounts[charcode] + count\n",
    "    charsetdf = pd.DataFrame({\"Code\" : [*charset.keys()], \"Count\" : [*charset.values()]})\n",
    "    charsetdf.sort_values(\"Count\", ascending=False, inplace=True)\n",
    "    charsetdf.reset_index(inplace=True)\n",
    "    charsetdf[\"Char\"] = charsetdf.index.map(lambda x:chr(x))\n",
    "    charsetdf[\"isAlpha\"] = charsetdf[\"Char\"].map(lambda x:x.isalpha())\n",
    "    charsetdf[\"isDigit\"] = charsetdf[\"Char\"].map(lambda x:x.isdigit())\n",
    "    charsetdf[\"isSpace\"] = charsetdf[\"Char\"].map(lambda x:x.isspace())\n",
    "    charsetdf[\"Percent\"] = 100*charsetdf[\"Count\"].cumsum()/charsetdf[\"Count\"].sum()\n",
    "    charsetfile = rootdir / \"charset.dataframe.feather\"\n",
    "    charsetdf.to_feather(charsetfile)\n",
    "    charsetdf.to_csv(rootdir / \"charset.csv\",sep=\";\")\n",
    "    print(f\"- {len(charset} distinct characters\")\n",
    "    return charsetdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Information about the vocabulary (distinct words) used in the extracted dataset :"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spacy",
   "language": "python",
   "name": "spacy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
